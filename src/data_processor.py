#!/usr/bin/env python3
"""
Myanmar Election Data Processor

This module handles the cleaning, processing, and transformation of Myanmar
election constituency data from Excel format to structured JSON/CSV.
"""

import pandas as pd
import json
import numpy as np
import re
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MyanmarElectionDataProcessor:
    """Process Myanmar election constituency data."""
    
    def __init__(self, data_dir: str = "data"):
        """Initialize the data processor.
        
        Args:
            data_dir: Base data directory path
        """
        self.data_dir = Path(data_dir)
        self.raw_dir = self.data_dir / "raw"
        self.processed_dir = self.data_dir / "processed"
        self.geojson_dir = self.data_dir / "geojson"
        
        # Create directories if they don't exist
        self.processed_dir.mkdir(exist_ok=True)
        
        # Myanmar state/region name mappings
        self.state_region_mapping = {
            # States
            "á€€á€á€»á€„á€ºá€•á€¼á€Šá€ºá€”á€šá€º": "Kachin State",
            "á€€á€šá€¬á€¸á€•á€¼á€Šá€ºá€”á€šá€º": "Kayah State", 
            "á€€á€›á€„á€ºá€•á€¼á€Šá€ºá€”á€šá€º": "Kayin State",
            "á€á€»á€„á€ºá€¸á€•á€¼á€Šá€ºá€”á€šá€º": "Chin State",
            "á€™á€½á€”á€ºá€•á€¼á€Šá€ºá€”á€šá€º": "Mon State",
            "á€›á€á€­á€¯á€„á€ºá€•á€¼á€Šá€ºá€”á€šá€º": "Rakhine State",
            "á€›á€¾á€™á€ºá€¸á€•á€¼á€Šá€ºá€”á€šá€º": "Shan State",
            # Regions (with different suffix variations)
            "á€…á€…á€ºá€€á€­á€¯á€„á€ºá€¸á€á€­á€¯á€„á€ºá€¸": "Sagaing Region",
            "á€…á€…á€ºá€€á€­á€¯á€„á€ºá€¸á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": "Sagaing Region",
            "á€á€”á€„á€ºá€¹á€á€¬á€›á€®á€á€­á€¯á€„á€ºá€¸": "Tanintharyi Region",
            "á€á€”á€„á€ºá€¹á€á€¬á€›á€®á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": "Tanintharyi Region",
            "á€•á€²á€á€°á€¸á€á€­á€¯á€„á€ºá€¸": "Bago Region",
            "á€•á€²á€á€°á€¸á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": "Bago Region",
            "á€™á€€á€½á€±á€¸á€á€­á€¯á€„á€ºá€¸": "Magway Region",
            "á€™á€€á€½á€±á€¸á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": "Magway Region",
            "á€™á€”á€¹á€á€œá€±á€¸á€á€­á€¯á€„á€ºá€¸": "Mandalay Region",
            "á€™á€”á€¹á€á€œá€±á€¸á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": "Mandalay Region",
            "á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸": "Yangon Region",
            "á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": "Yangon Region",
            "á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸": "Ayeyarwady Region",
            "á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": "Ayeyarwady Region",
            # Union Territory
            "á€”á€±á€•á€¼á€Šá€ºá€á€±á€¬á€ºá€•á€¼á€Šá€ºá€‘á€±á€¬á€„á€ºá€…á€¯á€”á€šá€ºá€™á€¼á€±": "Naypyitaw Union Territory",
            "á€•á€¼á€Šá€ºá€‘á€±á€¬á€„á€ºá€…á€¯á€”á€šá€ºá€™á€¼á€±": "Naypyitaw Union Territory"
        }
        
    def load_raw_data(self, filename: str = "MAL-ELECTION-PLAN.xlsx") -> pd.DataFrame:
        """Load raw Excel data and perform initial cleaning.
        
        Args:
            filename: Excel file name in raw data directory
            
        Returns:
            DataFrame with raw data
        """
        file_path = self.raw_dir / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"Data file not found: {file_path}")
            
        logger.info(f"Loading raw data from {file_path}")
        
        # Read Excel file, skipping Myanmar headers
        df_raw = pd.read_excel(file_path, header=None)
        
        # Find where actual data starts (first row with numeric ID)
        data_start_row = None
        for i, row in df_raw.iterrows():
            if pd.notna(row.iloc[0]) and str(row.iloc[0]).strip().isdigit():
                data_start_row = i
                break
                
        if data_start_row is None:
            raise ValueError("Could not find data start row")
            
        # Read with proper header
        df = pd.read_excel(file_path, skiprows=data_start_row-1)
        
        logger.info(f"Loaded {len(df)} constituency records")
        return df
        
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clean and standardize the data.
        
        Args:
            df: Raw DataFrame
            
        Returns:
            Cleaned DataFrame
        """
        logger.info("Cleaning and standardizing data")
        
        # Rename columns to English
        column_mapping = {
            df.columns[0]: "id",
            "á€á€­á€¯á€„á€ºá€¸/á€•á€¼á€Šá€ºá€”á€šá€º": "state_region_mm",
            "á€™á€²á€†á€”á€¹á€’á€”á€šá€º": "constituency_mm", 
            df.columns[3]: "assembly_type_mm",
            "á€™á€²á€†á€”á€¹á€’á€”á€šá€ºá€™á€¼á€±á€á€½á€„á€ºá€•á€«á€á€„á€ºá€á€Šá€·á€ºá€”á€šá€ºá€™á€¼á€±á€™á€»á€¬á€¸": "areas_included_mm",
            "á€€á€­á€¯á€šá€ºá€…á€¬á€¸á€œá€¾á€šá€º": "representatives",
            "á€™á€²á€…á€”á€…á€º": "electoral_system_mm"
        }
        
        df_clean = df.rename(columns=column_mapping)
        
        # Remove rows with missing critical data
        df_clean = df_clean.dropna(subset=["id", "state_region_mm", "constituency_mm"])
        
        # Convert ID to integer and ensure uniqueness
        df_clean["id"] = pd.to_numeric(df_clean["id"], errors="coerce")
        df_clean = df_clean.dropna(subset=["id"])
        df_clean["id"] = df_clean["id"].astype(int)
        
        # Remove duplicate IDs (keep first occurrence)
        df_clean = df_clean.drop_duplicates(subset=["id"], keep="first")
        
        # Convert representatives to integer
        df_clean["representatives"] = pd.to_numeric(df_clean["representatives"], errors="coerce").fillna(1).astype(int)
        
        # Add English translations
        df_clean["state_region_en"] = df_clean["state_region_mm"].map(self.state_region_mapping)
        
        # Clean constituency names (remove township suffix for cleaner English names)
        df_clean["constituency_en"] = df_clean["constituency_mm"].str.replace("á€™á€¼á€­á€¯á€·á€”á€šá€º", " Township")
        
        # Standardize assembly type
        df_clean["assembly_type"] = "pyithu"  # All records are Pyithu Hluttaw
        
        # Standardize electoral system
        df_clean["electoral_system"] = "FPTP"
        
        # Clean areas included text
        df_clean["areas_included_en"] = df_clean["areas_included_mm"].str.replace("á€™á€¼á€­á€¯á€·á€”á€šá€ºá€á€…á€ºá€á€¯á€œá€¯á€¶á€¸", "Entire Township")
        df_clean["areas_included_en"] = df_clean["areas_included_en"].str.replace("á‹", ".")
        
        # Fill missing areas_included_en
        df_clean["areas_included_en"] = df_clean["areas_included_en"].fillna("Area description not available")
        
        # Remove unnecessary columns
        columns_to_keep = [
            "id", "state_region_mm", "state_region_en", "constituency_mm", 
            "constituency_en", "assembly_type", "areas_included_mm", 
            "areas_included_en", "representatives", "electoral_system"
        ]
        
        df_clean = df_clean[columns_to_keep]
        
        # Reset index
        df_clean = df_clean.reset_index(drop=True)
        
        # Data validation
        self._validate_data(df_clean)
        
        logger.info(f"Data cleaning completed. {len(df_clean)} records processed")
        return df_clean
        
    def _validate_data(self, df: pd.DataFrame) -> None:
        """Validate cleaned data for consistency and completeness.
        
        Args:
            df: Cleaned DataFrame
        """
        logger.info("Validating data quality")
        
        # Check for required fields
        required_fields = ["id", "state_region_mm", "constituency_mm", "representatives"]
        for field in required_fields:
            if df[field].isnull().any():
                logger.warning(f"Missing values found in required field: {field}")
                
        # Check ID uniqueness
        if df["id"].duplicated().any():
            raise ValueError("Duplicate IDs found in data")
            
        # Check representative count (should all be 1)
        if not (df["representatives"] == 1).all():
            logger.warning("Non-standard representative count found")
            
        # Check for unmapped state/regions
        unmapped = df[df["state_region_en"].isnull()]["state_region_mm"].unique()
        if len(unmapped) > 0:
            logger.warning(f"Unmapped state/regions found: {unmapped}")
            
        logger.info("Data validation completed")
        
    def add_coordinates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add approximate coordinates for constituencies.
        
        Note: This is a simplified implementation. In production,
        you would use a proper geocoding service or coordinate database.
        
        Args:
            df: DataFrame with constituency data
            
        Returns:
            DataFrame with added coordinate columns
        """
        logger.info("Adding approximate coordinates")
        
        # Simplified coordinate mapping for major cities/townships
        # In production, this would be more comprehensive
        coordinate_mapping = {
            # States
            "á€€á€á€»á€„á€ºá€•á€¼á€Šá€ºá€”á€šá€º": {"lat": 25.3848, "lng": 97.3962},
            "á€€á€šá€¬á€¸á€•á€¼á€Šá€ºá€”á€šá€º": {"lat": 19.3000, "lng": 97.2000},
            "á€€á€›á€„á€ºá€•á€¼á€Šá€ºá€”á€šá€º": {"lat": 16.8000, "lng": 98.0000},
            "á€á€»á€„á€ºá€¸á€•á€¼á€Šá€ºá€”á€šá€º": {"lat": 22.5000, "lng": 93.5000},
            "á€™á€½á€”á€ºá€•á€¼á€Šá€ºá€”á€šá€º": {"lat": 15.5000, "lng": 97.6000},
            "á€›á€á€­á€¯á€„á€ºá€•á€¼á€Šá€ºá€”á€šá€º": {"lat": 20.1500, "lng": 94.1000},
            "á€›á€¾á€™á€ºá€¸á€•á€¼á€Šá€ºá€”á€šá€º": {"lat": 21.0000, "lng": 98.0000},
            # Regions (both variations)
            "á€…á€…á€ºá€€á€­á€¯á€„á€ºá€¸á€á€­á€¯á€„á€ºá€¸": {"lat": 22.0000, "lng": 95.0000},
            "á€…á€…á€ºá€€á€­á€¯á€„á€ºá€¸á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": {"lat": 22.0000, "lng": 95.0000},
            "á€á€”á€„á€ºá€¹á€á€¬á€›á€®á€á€­á€¯á€„á€ºá€¸": {"lat": 12.1000, "lng": 98.6000},
            "á€á€”á€„á€ºá€¹á€á€¬á€›á€®á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": {"lat": 12.1000, "lng": 98.6000},
            "á€•á€²á€á€°á€¸á€á€­á€¯á€„á€ºá€¸": {"lat": 17.3000, "lng": 96.5000},
            "á€•á€²á€á€°á€¸á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": {"lat": 17.3000, "lng": 96.5000},
            "á€™á€€á€½á€±á€¸á€á€­á€¯á€„á€ºá€¸": {"lat": 20.1500, "lng": 94.9000},
            "á€™á€€á€½á€±á€¸á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": {"lat": 20.1500, "lng": 94.9000},
            "á€™á€”á€¹á€á€œá€±á€¸á€á€­á€¯á€„á€ºá€¸": {"lat": 21.9588, "lng": 96.0891},
            "á€™á€”á€¹á€á€œá€±á€¸á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": {"lat": 21.9588, "lng": 96.0891},
            "á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸": {"lat": 16.8661, "lng": 96.1951},
            "á€›á€”á€ºá€€á€¯á€”á€ºá€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": {"lat": 16.8661, "lng": 96.1951},
            "á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸": {"lat": 16.9000, "lng": 95.2000},
            "á€§á€›á€¬á€á€á€®á€á€­á€¯á€„á€ºá€¸á€’á€±á€á€€á€¼á€®á€¸": {"lat": 16.9000, "lng": 95.2000},
            # Union Territory
            "á€”á€±á€•á€¼á€Šá€ºá€á€±á€¬á€ºá€•á€¼á€Šá€ºá€‘á€±á€¬á€„á€ºá€…á€¯á€”á€šá€ºá€™á€¼á€±": {"lat": 19.7451, "lng": 96.1297},
            "á€•á€¼á€Šá€ºá€‘á€±á€¬á€„á€ºá€…á€¯á€”á€šá€ºá€™á€¼á€±": {"lat": 19.7451, "lng": 96.1297}
        }
        
        # Add coordinates based on state/region
        df["lat"] = df["state_region_mm"].map(lambda x: coordinate_mapping.get(x, {}).get("lat"))
        df["lng"] = df["state_region_mm"].map(lambda x: coordinate_mapping.get(x, {}).get("lng"))
        
        # Add small random offset for constituencies within same state
        # This prevents overlapping points on the map
        np.random.seed(42)  # For reproducible results
        df["lat"] = df["lat"] + np.random.normal(0, 0.1, len(df))
        df["lng"] = df["lng"] + np.random.normal(0, 0.1, len(df))
        
        logger.info("Coordinate assignment completed")
        return df
        
    def generate_summary_statistics(self, df: pd.DataFrame) -> Dict:
        """Generate summary statistics for the dataset.
        
        Args:
            df: Processed DataFrame
            
        Returns:
            Dictionary with summary statistics
        """
        logger.info("Generating summary statistics")
        
        stats = {
            "total_constituencies": len(df),
            "total_representatives": df["representatives"].sum(),
            "states_regions": {
                "total": df["state_region_en"].nunique(),
                "breakdown": df.groupby("state_region_en").agg({
                    "id": "count",
                    "representatives": "sum"
                }).rename(columns={"id": "constituencies"}).to_dict("index")
            },
            "assembly_types": df["assembly_type"].value_counts().to_dict(),
            "electoral_system": df["electoral_system"].value_counts().to_dict()
        }
        
        return stats
        
    def save_processed_data(self, df: pd.DataFrame, stats: Dict) -> None:
        """Save processed data in multiple formats.
        
        Args:
            df: Processed DataFrame
            stats: Summary statistics
        """
        logger.info("Saving processed data")
        
        # Convert numpy/pandas types to Python native types for JSON serialization
        df_json = df.copy()
        for col in df_json.columns:
            if df_json[col].dtype == 'int64':
                df_json[col] = df_json[col].astype(int)
            elif df_json[col].dtype == 'float64':
                df_json[col] = df_json[col].astype(float)
                
        # Save as JSON (for web applications)
        json_data = {
            "metadata": {
                "total_records": int(len(df)),
                "last_updated": pd.Timestamp.now().isoformat(),
                "data_source": "Myanmar Election Commission"
            },
            "statistics": self._convert_to_json_serializable(stats),
            "constituencies": df_json.to_dict("records")
        }
        
        with open(self.processed_dir / "myanmar_constituencies.json", "w", encoding="utf-8") as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
            
        # Save as CSV (for analysis)
        df.to_csv(self.processed_dir / "myanmar_constituencies.csv", index=False, encoding="utf-8")
        
        # Save summary statistics separately
        with open(self.processed_dir / "summary_statistics.json", "w", encoding="utf-8") as f:
            json.dump(self._convert_to_json_serializable(stats), f, ensure_ascii=False, indent=2)
            
        logger.info("Data saved successfully")
        
    def _convert_to_json_serializable(self, obj):
        """Convert numpy/pandas types to JSON serializable types."""
        if isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_json_serializable(v) for v in obj]
        elif isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        else:
            return obj
        
    def process_all(self) -> Tuple[pd.DataFrame, Dict]:
        """Run the complete data processing pipeline.
        
        Returns:
            Tuple of (processed DataFrame, summary statistics)
        """
        logger.info("Starting complete data processing pipeline")
        
        # Load and process data
        df_raw = self.load_raw_data()
        df_clean = self.clean_data(df_raw)
        df_with_coords = self.add_coordinates(df_clean)
        
        # Generate statistics
        stats = self.generate_summary_statistics(df_with_coords)
        
        # Save results
        self.save_processed_data(df_with_coords, stats)
        
        logger.info("Data processing pipeline completed successfully")
        return df_with_coords, stats


def main():
    """Main function to run data processing."""
    processor = MyanmarElectionDataProcessor()
    df, stats = processor.process_all()
    
    print(f"\nâœ… Processing completed successfully!")
    print(f"ğŸ“Š Total constituencies: {stats['total_constituencies']}")
    print(f"ğŸ›ï¸ Total representatives: {stats['total_representatives']}")
    print(f"ğŸ—ºï¸ States/Regions covered: {stats['states_regions']['total']}")
    
    print(f"\nğŸ“ Files created:")
    print(f"   â€¢ data/processed/myanmar_constituencies.json")
    print(f"   â€¢ data/processed/myanmar_constituencies.csv") 
    print(f"   â€¢ data/processed/summary_statistics.json")


if __name__ == "__main__":
    main()